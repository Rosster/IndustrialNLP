{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom.minidom import parse\n",
    "from math import log\n",
    "import xml\n",
    "import os\n",
    "import sys\n",
    "import csv \n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import math\n",
    "from tqdm import tqdm_notebook\n",
    "import xml.dom.minidom\n",
    "from collections import Counter\n",
    "data_path = r\"C:\\Programming\\Python\\Programs_NLP\\wikihowcom-20141208-current.xml\"\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 or a more recent version is required.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batches):\n",
    "\n",
    "        with open('temp.xml', 'w') as f:\n",
    "            f.write('<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.8/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.8/ http://www.mediawiki.org/xml/export-0.8.xsd\" version=\"0.8\" xml:lang=\"en\">\\n')\n",
    "            for item in batches:\n",
    "                f.write(\"%s\" % item.encode('utf8'))\n",
    "            f.close()\n",
    "\n",
    "        with open('temp.xml', 'a') as f:\n",
    "            f.write('</mediawiki>')\n",
    "            f.close()\n",
    "        data_dict = parse_data('test.xml')\n",
    "        keys = [key for key in data_dict]\n",
    "\n",
    "        posts = []\n",
    "        for key in keys:\n",
    "            posts.append(parse_(key, data_dict))\n",
    "\n",
    "        with open('temp.json', 'w') as outfile:\n",
    "            for pa in posts:\n",
    "                json.dump({ \"index\" : {\"_index\": \"wikihow\", \"_type\": \"page\", \"_id\": pa[\"id\"]}}, outfile)\n",
    "                outfile.write('\\n')\n",
    "                json.dump(pa, outfile)\n",
    "                outfile.write('\\n')\n",
    "            outfile.close()\n",
    "        bulk_data = []\n",
    "\n",
    "        for pa in posts:\n",
    "            bulk_data.append({\"index\" : {\"_index\": \"wikihow\", \"_type\": \"page\", \"_id\": pa[\"id\"]}})\n",
    "            bulk_data.append(json.dumps(pa, ensure_ascii=False))\n",
    "\n",
    "        print(\"Batch Number: \" + str(batch_count) + \" Processed\")\n",
    "        print(\"Indexing Batch Number: \" + str(batch_count) + \"...\")\n",
    "# \t\tconfig = {'host': 'localhost'}\n",
    "# \t\tes = elasticsearch.Elasticsearch([config,], timeout=300)\n",
    "# \t\tres = es.bulk(index = 'wikihow', body = bulk_data)\n",
    "        #print(res)\n",
    "        # check data is in there, and structure in there\n",
    "        #print(\"Test Import...\")\n",
    "        #print(json.dumps(es.search(body={\"query\": {\"match_all\": {}}}, index = 'wikihow')))\n",
    "\n",
    "def parse_data(file):\n",
    "    file_dict = {}\n",
    "    # Open XML document using minidom parser\n",
    "    DOMTree = xml.dom.minidom.parse(file)\n",
    "    mediawiki = DOMTree.documentElement\n",
    "    #if mediawiki.hasAttribute(\"xmlns\"):\n",
    "        #print(\"Root element : %s\" % mediawiki.getAttribute(\"xmlns\"))\n",
    "    # Get all the pages on file\n",
    "    pages = mediawiki.getElementsByTagName(\"page\")\n",
    "    # Get high level namespace information.\n",
    "    for page in pages:\n",
    "        page_dom = page.getElementsByTagName('text')[0]\n",
    "        dom_text = page_dom.childNodes[0].data\n",
    "        if str(dom_text[0]) == '#':\n",
    "            pass\n",
    "        else:\n",
    "            id_ = page.getElementsByTagName('id')[0]\n",
    "            id_ = id_.childNodes[0].data\n",
    "            title = page.getElementsByTagName('title')[0]\n",
    "            title = title.childNodes[0].data\n",
    "            text = page.getElementsByTagName('text')[0]\n",
    "            text = text.childNodes[0].data\n",
    "            file_dict[int(id_)] = [title, text]\n",
    "    return file_dict\n",
    "\n",
    "\n",
    "#data_dict = parse_data(data_path)\n",
    "\n",
    "def structure(page_id, file_dict):\n",
    "    file_ = file_dict[page_id][1]\n",
    "    lines = file_.split(\"\\n\")\n",
    "    lines_ = []\n",
    "    for line in lines:\n",
    "        if line != \"\":\n",
    "            lines_.append(line)\n",
    "    intro = lines_[0]\n",
    "    try:\n",
    "        if \"Category\" in lines_[1]:\n",
    "            cat = lines_[1]\n",
    "            if \":\" in cat:\n",
    "                cat = cat.split(\":\")[1][:-2]\n",
    "        else:\n",
    "            cat = None\n",
    "    except IndexError:\n",
    "        cat = None\n",
    "    try:\n",
    "        if \"Category\" in lines[2] and cat is not None:\n",
    "            cat = cat + \",\" + re.sub('[^A-Za-z0-9\\s\\.|\\:\\*]+', '', lines[2]).split(\":\")[1]\n",
    "            process = lines_[3:]\n",
    "        else:\n",
    "            process = lines_[2:]\n",
    "    except IndexError:\n",
    "        process = None\n",
    "\n",
    "    return intro, cat, process\n",
    "\n",
    "def merge(data_list):\n",
    "    try:\n",
    "        data_list = \" \".join(data_list)\n",
    "        data_list = data_list.split(\" ==\")\n",
    "        data_list = list(zip(*[iter(data_list)] * 2))\n",
    "    except TypeError:\n",
    "        data_list = []\n",
    "    return data_list\n",
    "\n",
    "def construct_dict(data_list):\n",
    "    textdict = {}\n",
    "    steps_dict = {}\n",
    "    #construct step dictt\n",
    "    try:\n",
    "        if data_list[0][1] != \"\":\n",
    "            title = re.sub(r'([^\\:\\w])+', '', data_list[0][0]) \n",
    "            #print(title)\n",
    "            steps = data_list[0][1].split(\" #\")\n",
    "            steps = [st for st in steps if st != \"\"]\n",
    "            step_index = [\"step\" + str(number + 1) for number in range(len(steps))]\n",
    "            step_data = [ste.split(\"<br><br>\") for ste in steps]\n",
    "            step_new =  []\n",
    "            for step in step_data:\n",
    "                step_new.append(step[0])  \n",
    "            step_data = step_new\n",
    "            steps_dict = dict(zip(step_index, step_data))\n",
    "            #return steps_dict\n",
    "            textdict[\"steps\"] = steps_dict\n",
    "        subsections = []\n",
    "        for sect in data_list[1:]:\n",
    "            if sect == \"\":\n",
    "                sect.pop(0)\n",
    "                #print(sect)\n",
    "            title = sect[0]\n",
    "            title = re.sub(r'([^\\:\\w_])+', '', title)\n",
    "            title  = \"\".join([t for t in title if t != \" \"])\n",
    "            #print(sect[1:])\n",
    "            content = sect[1]\n",
    "            if content[1] == \"*\":\n",
    "                subs = content.split(\"*\")\n",
    "                subs = [s for s in subs if s != \" \"]\n",
    "                subs = [re.sub('[^A-Za-z0-9\\s\\.|\\:\\*]+', '', string) for string in subs]\n",
    "                textdict[title] = subs\n",
    "        return textdict\n",
    "    except IndexError:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def parse_(page_id, data_dict):\n",
    "    structured = structure(page_id, data_dict)\n",
    "    final_dict = construct_dict(merge(structured[2]))\n",
    "    final_dict[\"title\"] = data_dict[page_id][0]\n",
    "    final_dict[\"introduction\"] = structured[0]\n",
    "    final_dict[\"category\"] = structured[1]\n",
    "    final_dict[\"id\"] = page_id\n",
    "    #parsed = json.loads(json.dumps(final_dict, ensure_ascii=False, sort_keys=True))\n",
    "    #print(json.dumps(parsed, indent=4, sort_keys=True))\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data (this will tax your memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = parse_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = []\n",
    "for val in data_dict.values():\n",
    "    cat = re.search(r'(?<=\\[\\[category:)[^\\s\\\\\\]]*',val[1],flags=re.I)\n",
    "    if cat is not None:\n",
    "        cat_list.append(cat.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_counter = Counter(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode(regex, sep=None):\n",
    "        if regex is None:\n",
    "            return None\n",
    "        else:\n",
    "            if sep is None:\n",
    "                return regex.group(0)\n",
    "            else:\n",
    "                return [w.strip() for w in regex.group(0).split(sep) if w != '']\n",
    "    \n",
    "    def __init__(self,entry):\n",
    "        section_regex = lambda section: r'(?<= {} \\=\\=)[\\s\\S]+?(?=\\=\\= |\\Z)'.format(section)\n",
    "        \n",
    "        self.title = entry[0]\n",
    "        self.text = entry[1]\n",
    "        cat = re.search(r'(?<=\\[\\[category:)[^\\s\\\\\\]]*',val[1],flags=re.I)\n",
    "        self.category =  cat.group(0) if cat is not None else None\n",
    "        self.intro = Post.decode(re.search(r'[\\s\\S]+(?=\\[\\[category)',self.text,re.M|re.I))\n",
    "        self.steps = Post.decode(re.search(section_regex('steps'),self.text,re.M | re.I), sep='\\n#')\n",
    "        self.tips = Post.decode(re.search(section_regex('tips'),self.text,re.M | re.I), sep='\\n*')\n",
    "        self.warnings = Post.decode(re.search(section_regex('warnings'),self.text,re.M | re.I), sep='\\n*')\n",
    "        self.tools = Post.decode(re.search(section_regex('you\\'ll need'),self.text,re.M | re.I), sep='\\n*')\n",
    "        self.related_posts = Post.decode(re.search(section_regex('related wikihows'),self.text,re.M | re.I), sep='\\n*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e0e657838440dd93d6a147bc312989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=198150), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method tqdm.__del__ of   1%|â–‰                                                                        | 2382/198150 [15:35<21:21:37,  2.55it/s]>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 878, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 1097, in close\n",
      "    self._decr_instances(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 451, in _decr_instances\n",
      "    cls.monitor.exit()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py\", line 50, in exit\n",
      "    self.join()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 1053, in join\n",
      "    raise RuntimeError(\"cannot join current thread\")\n",
      "RuntimeError: cannot join current thread\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "post_object_list = []\n",
    "for val in tqdm_notebook(data_dict.values()):\n",
    "    post_object_list.append(Post(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( post_object_list, open( \"posts.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
