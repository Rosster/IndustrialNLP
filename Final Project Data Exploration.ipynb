{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom.minidom import parse\n",
    "from math import log\n",
    "import xml\n",
    "import os\n",
    "import sys\n",
    "import csv \n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import math\n",
    "# from tqdm import tqdm_notebook\n",
    "import xml.dom.minidom\n",
    "from collections import Counter\n",
    "data_path = r\"C:\\Programming\\Python\\Programs_NLP\\wikihowcom-20141208-current.xml\"\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 or a more recent version is required.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file):\n",
    "    file_dict = {}\n",
    "    # Open XML document using minidom parser\n",
    "    DOMTree = xml.dom.minidom.parse(file)\n",
    "    mediawiki = DOMTree.documentElement\n",
    "    #if mediawiki.hasAttribute(\"xmlns\"):\n",
    "        #print(\"Root element : %s\" % mediawiki.getAttribute(\"xmlns\"))\n",
    "    # Get all the pages on file\n",
    "    pages = mediawiki.getElementsByTagName(\"page\")\n",
    "    # Get high level namespace information.\n",
    "    for page in pages:\n",
    "        page_dom = page.getElementsByTagName('text')[0]\n",
    "        dom_text = page_dom.childNodes[0].data\n",
    "        if str(dom_text[0]) == '#':\n",
    "            pass\n",
    "        else:\n",
    "            id_ = page.getElementsByTagName('id')[0]\n",
    "            id_ = id_.childNodes[0].data\n",
    "            title = page.getElementsByTagName('title')[0]\n",
    "            title = title.childNodes[0].data\n",
    "            text = page.getElementsByTagName('text')[0]\n",
    "            text = text.childNodes[0].data\n",
    "            file_dict[int(id_)] = [title, text]\n",
    "    return file_dict\n",
    "\n",
    "\n",
    "#data_dict = parse_data(data_path)\n",
    "\n",
    "def structure(page_id, file_dict):\n",
    "    file_ = file_dict[page_id][1]\n",
    "    lines = file_.split(\"\\n\")\n",
    "    lines_ = []\n",
    "    for line in lines:\n",
    "        if line != \"\":\n",
    "            lines_.append(line)\n",
    "    intro = lines_[0]\n",
    "    try:\n",
    "        if \"Category\" in lines_[1]:\n",
    "            cat = lines_[1]\n",
    "            if \":\" in cat:\n",
    "                cat = cat.split(\":\")[1][:-2]\n",
    "        else:\n",
    "            cat = None\n",
    "    except IndexError:\n",
    "        cat = None\n",
    "    try:\n",
    "        if \"Category\" in lines[2] and cat is not None:\n",
    "            cat = cat + \",\" + re.sub('[^A-Za-z0-9\\s\\.|\\:\\*]+', '', lines[2]).split(\":\")[1]\n",
    "            process = lines_[3:]\n",
    "        else:\n",
    "            process = lines_[2:]\n",
    "    except IndexError:\n",
    "        process = None\n",
    "\n",
    "    return intro, cat, process\n",
    "\n",
    "def merge(data_list):\n",
    "    try:\n",
    "        data_list = \" \".join(data_list)\n",
    "        data_list = data_list.split(\" ==\")\n",
    "        data_list = list(zip(*[iter(data_list)] * 2))\n",
    "    except TypeError:\n",
    "        data_list = []\n",
    "    return data_list\n",
    "\n",
    "def construct_dict(data_list):\n",
    "    textdict = {}\n",
    "    steps_dict = {}\n",
    "    #construct step dictt\n",
    "    try:\n",
    "        if data_list[0][1] != \"\":\n",
    "            title = re.sub(r'([^\\:\\w])+', '', data_list[0][0]) \n",
    "            #print(title)\n",
    "            steps = data_list[0][1].split(\" #\")\n",
    "            steps = [st for st in steps if st != \"\"]\n",
    "            step_index = [\"step\" + str(number + 1) for number in range(len(steps))]\n",
    "            step_data = [ste.split(\"<br><br>\") for ste in steps]\n",
    "            step_new =  []\n",
    "            for step in step_data:\n",
    "                step_new.append(step[0])  \n",
    "            step_data = step_new\n",
    "            steps_dict = dict(zip(step_index, step_data))\n",
    "            #return steps_dict\n",
    "            textdict[\"steps\"] = steps_dict\n",
    "        subsections = []\n",
    "        for sect in data_list[1:]:\n",
    "            if sect == \"\":\n",
    "                sect.pop(0)\n",
    "                #print(sect)\n",
    "            title = sect[0]\n",
    "            title = re.sub(r'([^\\:\\w_])+', '', title)\n",
    "            title  = \"\".join([t for t in title if t != \" \"])\n",
    "            #print(sect[1:])\n",
    "            content = sect[1]\n",
    "            if content[1] == \"*\":\n",
    "                subs = content.split(\"*\")\n",
    "                subs = [s for s in subs if s != \" \"]\n",
    "                subs = [re.sub('[^A-Za-z0-9\\s\\.|\\:\\*]+', '', string) for string in subs]\n",
    "                textdict[title] = subs\n",
    "        return textdict\n",
    "    except IndexError:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def parse_(page_id, data_dict):\n",
    "    structured = structure(page_id, data_dict)\n",
    "    final_dict = construct_dict(merge(structured[2]))\n",
    "    final_dict[\"title\"] = data_dict[page_id][0]\n",
    "    final_dict[\"introduction\"] = structured[0]\n",
    "    final_dict[\"category\"] = structured[1]\n",
    "    final_dict[\"id\"] = page_id\n",
    "    #parsed = json.loads(json.dumps(final_dict, ensure_ascii=False, sort_keys=True))\n",
    "    #print(json.dumps(parsed, indent=4, sort_keys=True))\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data (this will tax your memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = parse_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = []\n",
    "for val in data_dict.values():\n",
    "    cat = re.search(r'(?<=\\[\\[category:)[^\\s\\\\\\]]*',val[1],flags=re.I)\n",
    "    if cat is not None:\n",
    "        cat_list.append(cat.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_counter = Counter(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode(regex, sep=None):\n",
    "        if regex is None:\n",
    "            return None\n",
    "        else:\n",
    "            if sep is None:\n",
    "                return regex.group(0)\n",
    "            else:\n",
    "                return [w.strip() for w in regex.group(0).split(sep) if w != '']\n",
    "    \n",
    "    def __init__(self,entry):\n",
    "        section_regex = lambda section: r'(?<= {} \\=\\=)[\\s\\S]+?(?=\\=\\= |\\Z)'.format(section)\n",
    "        \n",
    "        self.title = entry[0]\n",
    "        self.text = entry[1]\n",
    "        cat = re.search(r'(?<=\\[\\[category:)[^\\s\\\\\\]]*',val[1],flags=re.I)\n",
    "        self.category =  cat.group(0) if cat is not None else None\n",
    "        self.intro = Post.decode(re.search(r'[\\s\\S]+(?=\\[\\[category)',self.text,re.M|re.I))\n",
    "        self.steps = Post.decode(re.search(section_regex('steps'),self.text,re.M | re.I), sep='\\n#')\n",
    "        self.tips = Post.decode(re.search(section_regex('tips'),self.text,re.M | re.I), sep='\\n*')\n",
    "        self.warnings = Post.decode(re.search(section_regex('warnings'),self.text,re.M | re.I), sep='\\n*')\n",
    "        self.tools = Post.decode(re.search(section_regex('you\\'ll need'),self.text,re.M | re.I), sep='\\n*')\n",
    "        self.related_posts = Post.decode(re.search(section_regex('related wikihows'),self.text,re.M | re.I), sep='\\n*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_object_list = []\n",
    "for val in data_dict.values():\n",
    "    post_object_list.append(Post(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( post_object_list, open( \"posts.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
